---
title: "Homework Assignment 4"
author: "Shiyi Li"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---

# NYC Crash Data Exploration

Except for the first question, use the cleaned crash data in feather format.

    a.  Construct a contigency table for missing in geocode (latitude and
     longitude) by borough. Is the missing pattern the same across boroughs?
     Formulate a hypothesis and test it. 

```{python}
import pandas as pd
import numpy as np
import rpy2.robjects.numpy2ri
from rpy2.robjects.packages import importr
rpy2.robjects.numpy2ri.activate()

stats = importr('stats')

# Load the cleaned dataset exported from the 'Homework 4 - NYC Data Cleaning' 
# assignment
file_path = 'data/shiyili_cleaned_data.csv'
df_shiyili = pd.read_csv(file_path)
df_shiyili['borough'] = df_shiyili['borough'].apply(lambda x: str(x).upper())

# Create a 'geo_missing' column to show where rows are missing latitude and 
# longitude.
df_shiyili['geo_missing'] = np.where(df_shiyili[[
  'latitude', 
  'longitude']].isnull().all(axis=1)==True, 1, 0)

# Construct contingency table for missing geocodes by borough
contingency_table = pd.crosstab(
  df_shiyili['geo_missing'], 
  df_shiyili['borough']).drop(columns=['NAN'], errors='ignore')

print(contingency_table)

# Perform Fisher's exact test for independence
res = stats.fisher_test(contingency_table.to_numpy(), simulate_p_value = True)
print(res)

# Interpret results
alpha = 0.05  # Significance level
p = res[0][0]
if p < alpha:
    print("Reject the null hypothesis: Missing geocodes are NOT independent of boroughs.")
else:
    print("Fail to reject the null hypothesis: No strong evidence that missing geocodes depend on boroughs.")
```

    a.  Construct a `hour` variable with integer values from 0 to 23. Plot the
     histogram of the number of crashes by `hour`. Plot it by borough.

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Load the cleaned dataset
df = pd.read_feather("data/nyccrashes_cleaned.feather")

# Create a 'hour' column to store the hours extracted from the crash_datetime 
# column
df['hour'] = pd.to_datetime(df['crash_datetime']).dt.hour

# Plot histogram of number of crashes by hour (all boroughs combined)
plt.figure(figsize=(10, 5))
plt.hist(df['hour'], bins=24, edgecolor='black', alpha=0.7)
plt.xlabel("Hour of the Day")
plt.ylabel("Number of Crashes")
plt.title("Histogram of NYC Crashes by Hour")
plt.xticks(range(0, 24))  # Ensure all hours are labeled
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Get unique boroughs
boroughs = df['borough'].dropna().unique()

# Define subplot grid size
n_rows = 2
n_cols = 3
fig, ax = plt.subplots(n_rows, n_cols, figsize=(20, 12)) 
ax = ax.flatten()  # Flatten the axes array for easier iteration

# Plot histogram of crashes by hour for each borough
for idx, borough_name in enumerate(boroughs):
    x = df[df['borough'] == borough_name]['hour']
    ax[idx].hist(x, bins=24, alpha=0.7, edgecolor='black')
    ax[idx].set_title(f'Histogram of NYC Crashes by Hour for {borough_name}')
    ax[idx].set_xlabel("Hour of the Day")
    ax[idx].set_ylabel("Number of Crashes")
    ax[idx].set_xticks(range(0, 24))
    ax[idx].grid(axis='y', linestyle='--', alpha=0.7)

# Hide unused subplots if boroughs < 6
for i in range(len(boroughs), len(ax)):
    fig.delaxes(ax[i])

plt.tight_layout()
plt.show()

```
     
    a.  Overlay the locations of the crashes on a map of NYC. The map could be a
     static map or a Google map.

```{python}
# Load Required Libraries
import os
import io
import zipfile
import requests
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

# Define the NYC MODZCTA shapefile URL and extraction directory
shapefile_url = "https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile"
extract_dir = "MODZCTA_Shapefile"

# Create the directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Step 1: Download and extract the shapefile
print("Downloading MODZCTA shapefile...")
response = requests.get(shapefile_url)
with zipfile.ZipFile(io.BytesIO(response.content), "r") as z:
    z.extractall(extract_dir)

print(f"Shapefile extracted to: {extract_dir}")
```

```{python}
# Step 2: Automatically detect the correct .shp file
shapefile_path = None
for file in os.listdir(extract_dir):
    if file.endswith(".shp"):
        shapefile_path = os.path.join(extract_dir, file)
        break  # Use the first .shp file found

if not shapefile_path:
    raise FileNotFoundError("No .shp file found in extracted directory.")

print(f"Using shapefile: {shapefile_path}")

# Step 3: Load the shapefile into GeoPandas
gdf_nyc = gpd.read_file(shapefile_path)

# Step 4: Convert to CRS with latitude/longitude (WGS 84) for correct mapping
gdf_nyc = gdf_nyc.to_crs(epsg=4326)

print(gdf_nyc.head())
```

```{python}
# Remove missing values for latitude and longitude
df_crashes = df.dropna(subset=['latitude', 'longitude'])

# Convert to GeoDataFrame
gdf_crashes = gpd.GeoDataFrame(df_crashes, geometry=gpd.points_from_xy(
                                df_crashes.longitude, 
                                df_crashes.latitude
                                ), crs="EPSG:4326")

print(gdf_crashes[['zip_code', 'latitude', 'longitude']])
```

```{python}
# Set up figure and axis
fig, ax = plt.subplots(figsize=(10, 12))

# Plot NYC boundary map
gdf_nyc.plot(ax=ax, cmap='viridis', 
linewidth=0.8, 
edgecolor='black',
legend=True, 
label="NYC Boundaries")

# Overlay crash locations as red dots
gdf_crashes.plot(
  ax=ax, color='red', markersize=2, alpha=0.6, label="Crash Locations"
  )

# Add title and legend
ax.set_title("NYC Crash Locations Overlay on NYC Map", fontsize=14)
ax.legend()

# Remove axes
ax.set_xticks([])
ax.set_yticks([])
ax.set_frame_on(False)

# Show plot
plt.show()
```

    a.  Create a new variable `severe` which is one if the number of persons
     injured or deaths is 1 or more; and zero otherwise. Construct a cross
     table for `severe` versus borough. Is the severity of the crashes the
     same across boroughs? Test the null hypothesis that the two variables
     are not associated with an appropriate test.

```{python}
import pandas as pd
import numpy as np
import rpy2.robjects.numpy2ri
from rpy2.robjects.packages import importr
rpy2.robjects.numpy2ri.activate()

stats = importr('stats')

# Create 'severe' column (1 if at least one injury or death, else 0)
df['severe'] = np.where(
  (df['number_of_persons_injured'] > 0) | 
  (df['number_of_persons_killed'] > 0), 1, 0)

# Construct a cross table for `severe` versus borough
contingency_table_2 = pd.crosstab(df['severe'], df['borough'])

print(contingency_table_2)

# Perform Fisher's exact test for independence
res_2 = stats.fisher_test(
  contingency_table_2.to_numpy(), 
  simulate_p_value = True)

print(res_2)

# Interpret results
alpha = 0.05  # Significance level
p_2 = res_2[0][0]
if p_2 < alpha:
    print("Reject the null hypothesis: Crash severity is NOT independent of borough.")
else:
    print("Fail to reject the null hypothesis: No strong evidence that crash severity depends on borough.")
```

    a.  Merge the crash data with the Census zip code database which
   contains zip-code level demographic or socioeconomic variables.

```{python}
# List of valid NYC ZIP codes compiled from UHF codes
# Define all_valid_zips based on the earlier extracted ZIP codes
all_valid_zips = {
    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,
    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,
    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,
    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,
    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,
    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,
    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,
    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,
    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,
    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,
    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,
    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,
    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,
    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,
    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,
    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,
    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,
    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,
    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,
    10306, 10307, 10308, 10309, 10312
}

```

```{python}
# Import modules
import matplotlib.pyplot as plt
import pandas as pd
import geopandas as gpd
from census import Census
from us import states
import os
import io

api_key = open("censusAPIkey.txt").read().strip()
c = Census(api_key)
```

```{python}
ACS_YEAR = 2023
ACS_DATASET = "acs/acs5"

# Important ACS variables (including land area for density calculation)
ACS_VARIABLES = {
    "B01003_001E": "Total Population",
    "B19013_001E": "Median Household Income",
    "B02001_002E": "White Population",
    "B02001_003E": "Black Population",
    "B02001_005E": "Asian Population",
    "B15003_022E": "Bachelorâ€™s Degree Holders",
    "B15003_025E": "Graduate Degree Holders",
    "B23025_002E": "Labor Force",
    "B23025_005E": "Unemployed",
    "B25077_001E": "Median Home Value"
}

# Convert set to list of strings
all_valid_zips = list(map(str, all_valid_zips))
```

```{python}
acs_data = c.acs5.get(
    list(ACS_VARIABLES.keys()), 
    {'for': f'zip code tabulation area:{",".join(all_valid_zips)}'}
    )

# Convert to DataFrame
df_acs = pd.DataFrame(acs_data)

# Rename columns
df_acs.rename(columns=ACS_VARIABLES, inplace=True)
df_acs.rename(columns={"zip code tabulation area": "ZIP Code"}, inplace=True)

# Check the data types of the columns in 'df_acs'
df_acs.dtypes

# Check the data types of the columns in 'df'
df.dtypes

# Convert ZIP codes to integers first, then to strings
df['zip_code'] = pd.to_numeric(
  df['zip_code'], 
  errors='coerce'
  ).fillna(0).astype(int).astype(str)

# Ensure all ZIP codes are 5-digit strings
df['zip_code'] = df['zip_code'].str.zfill(5)

# Do the same for df_acs
df_acs['ZIP Code'] = df_acs['ZIP Code'].astype(str).str.zfill(5)

# Merge crash data with Census data using ZIP Code as the key
df_merged = df.merge(
  df_acs, left_on='zip_code', right_on='ZIP Code', how='left'
  )

# Check the merged dataframe
print(df_merged.head(20))

# Count missing census data
missing_census_data = df_merged[df_merged['Total Population'].isnull()]
print("Number of crashes with missing census data:", len(missing_census_data))

# Check all crash ZIP codes exist in the Census data
missing_zips = set(df['zip_code'].unique()) - set(df_acs['ZIP Code'].unique())
print("ZIP Codes in crashes but not in Census data:", missing_zips)

# Check all Census ZIP codes exist in the crash data
missing_census_zips = set(
  df_acs['ZIP Code'].unique()
  ) - set(
    df['zip_code'].unique()
    )
print("ZIP Codes in Census but not in crashes:", missing_census_zips)
```

    a.  Fit a logistic model with `severe` as the outcome variable and covariates
     that are available in the data or can be engineered from the data. For
     example, zip code level covariates obtained from merging with the
     zip code database; crash hour; number of vehicles involved.

```{python}
# Initial selection of columns to be used
columns_to_use = [
  "severe", "borough", "latitude", "longitude", "number_of_persons_injured",
  "number_of_persons_killed", "number_of_pedestrians_injured", 
  "number_of_pedestrians_killed", "number_of_cyclist_injured", 
  "number_of_motorist_injured", "number_of_motorist_killed",
  "hour", "Total Population", "Median Household Income", 
  "White Population", "Black Population", "Asian Population", 
  "Bachelorâ€™s Degree Holders", "Graduate Degree Holders", "Labor Force", 
  "Unemployed", "Median Home Value"
  ]

# Create a DataFrame named `simdat` including all the features we need to use 
simdat = df_merged[columns_to_use]

# Check for the shape of the dataframe 'simdat'
print(simdat.shape)

# Check for missing values 
print(simdat.isnull().sum())

# Drop all the rows with missing values in the dataframe 'simdat'
simdat = simdat.dropna()

# Check for the shape of the dataframe 'simdat'
print(simdat.shape)

# Check for the data types of each variable in the data frame 'simdat'
print(simdat.dtypes)

# Using one hot encoding/dummy variables to address the category variable 
# 'borough'
simdat = pd.get_dummies(simdat, columns = ['borough']) 
print(simdat)

# Rename columns to replace spaces with underscores
simdat.columns = simdat.columns.str.replace(" ", "_").str.replace("-", "_")
```

```{python}
# Compute correlation matrix
correlation_matrix = simdat.corr()

# Get absolute correlation with 'severe'
correlation_with_severe = correlation_matrix['severe'].abs().sort_values(
  ascending=False
  )

# Set a threshold (e.g., 0.01) to filter relevant variables
threshold = 0.01
highly_correlated_vars = correlation_with_severe[
  correlation_with_severe > threshold
  ].index.tolist()

# Remove 'severe' itself from the list
highly_correlated_vars.remove('severe')

# Display selected variables
print("Variables with high correlation with 'severe':")
print(highly_correlated_vars)

# Prepare data for logistic regression
X = simdat[highly_correlated_vars]  # Features
y = simdat['severe']  # Target variable
```

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Reformat column names in `highly_correlated_vars` as well
highly_correlated_vars = [
  col.replace(" ", "_").replace("-", "_") for col in highly_correlated_vars
  ]

# Define the logistic regression formula
formula = "severe ~ " + " + ".join(highly_correlated_vars)

# Add intercept manually for statsmodels
X = sm.add_constant(X)

# Fit logistic regression using formula API
fit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()

# Display model summary
print(fit.summary())
```

```{python}
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, confusion_matrix,
    f1_score, roc_curve, auc
)
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=['const']), y, test_size=0.25, random_state=42)

# Fit the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict labels on the test set
y_pred = model.predict(X_test)

# Get predicted probabilities for ROC curve and AUC
y_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate accuracy, precision, and recall
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# Print confusion matrix and metrics
print("Confusion Matrix:\n", cm)
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
```
