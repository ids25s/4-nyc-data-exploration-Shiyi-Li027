---
title: "Homework Assignment 4"
author: "Shiyi Li"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---

# NYC Crash Data Exploration

Except for the first question, use the cleaned crash data in feather format.

    a.  Construct a contigency table for missing in geocode (latitude and
     longitude) by borough. Is the missing pattern the same across boroughs?
     Formulate a hypothesis and test it. 

```{python}
import pandas as pd
import numpy as np
import rpy2.robjects.numpy2ri
from rpy2.robjects.packages import importr
rpy2.robjects.numpy2ri.activate()

stats = importr('stats')

# Load the cleaned dataset exported from the 'Homework 4 - NYC Data Cleaning' assignment
file_path = 'data/shiyili_cleaned_data.csv'
df_shiyili = pd.read_csv(file_path)

# Create a 'geo_missing' column to show where rows are missing latitude and longitude.
df_shiyili['geo_missing'] = np.where(df_shiyili[['latitude', 'longitude']].isnull().all(axis=1)==True, 1, 0)

# Construct contingency table for missing geocodes by borough
contingency_table = pd.crosstab(df_shiyili['geo_missing'], df_shiyili['borough'])
print(contingency_table)

# Perform Fisher's exact test for independence
res = stats.fisher_test(contingency_table.to_numpy(), simulate_p_value = True)
print(res)

# Interpret results
alpha = 0.05  # Significance level
p = res[0][0]
if p < alpha:
    print("Reject the null hypothesis: Missing geocodes are NOT independent of boroughs.")
else:
    print("Fail to reject the null hypothesis: No strong evidence that missing geocodes depend on boroughs.")
```

    a.  Construct a `hour` variable with integer values from 0 to 23. Plot the
     histogram of the number of crashes by `hour`. Plot it by borough.

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Load the cleaned dataset
df = pd.read_feather("data/nyccrashes_cleaned.feather")

# Create a 'hour' column to store the hours extracted from the crash_datetime column
df['hour'] = pd.to_datetime(df['crash_datetime']).dt.hour

# Plot histogram of number of crashes by hour (all boroughs combined)
plt.figure(figsize=(10, 5))
plt.hist(df['hour'], bins=24, edgecolor='black', alpha=0.7)
plt.xlabel("Hour of the Day")
plt.ylabel("Number of Crashes")
plt.title("Histogram of NYC Crashes by Hour")
plt.xticks(range(0, 24))  # Ensure all hours are labeled
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Plot histogram of crashes by hour for each borough

plt.figure(figsize=(12, 6))
for borough in df['borough'].dropna().unique():
    subset = df[df['borough'] == borough]
    plt.hist(subset['hour'], bins=24, alpha=0.5, label=borough)

plt.xlabel("Hour of the Day")
plt.ylabel("Number of Crashes")
plt.title("Histogram of NYC Crashes by Hour and Borough")
plt.xticks(range(0, 24))
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Get unique boroughs
boroughs = df['borough'].dropna().unique()

# Define subplot grid size
n_rows = 2
n_cols = 3
fig, ax = plt.subplots(n_rows, n_cols, figsize=(20, 12))  # Adjusted for 6 boroughs max
ax = ax.flatten()  # Flatten the axes array for easier iteration

# Plot histogram of crashes by hour for each borough
for idx, borough_name in enumerate(boroughs):
    x = df[df['borough'] == borough_name]['hour']
    ax[idx].hist(x, bins=24, alpha=0.7, edgecolor='black')
    ax[idx].set_title(f'Histogram of NYC Crashes by Hour for {borough_name}')
    ax[idx].set_xlabel("Hour of the Day")
    ax[idx].set_ylabel("Number of Crashes")
    ax[idx].set_xticks(range(0, 24))
    ax[idx].grid(axis='y', linestyle='--', alpha=0.7)

# Hide unused subplots if boroughs < 6
for i in range(len(boroughs), len(ax)):
    fig.delaxes(ax[i])

plt.tight_layout()
plt.show()

```
     
    a.  Overlay the locations of the crashes on a map of NYC. The map could be a
     static map or a Google map.

```{python}
# Load Required Libraries
import os
import io
import zipfile
import requests
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

# Define the NYC MODZCTA shapefile URL and extraction directory
shapefile_url = "https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile"
extract_dir = "MODZCTA_Shapefile"

# Create the directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Step 1: Download and extract the shapefile
print("Downloading MODZCTA shapefile...")
response = requests.get(shapefile_url)
with zipfile.ZipFile(io.BytesIO(response.content), "r") as z:
    z.extractall(extract_dir)

print(f"Shapefile extracted to: {extract_dir}")
```

```{python}
# Step 2: Automatically detect the correct .shp file
shapefile_path = None
for file in os.listdir(extract_dir):
    if file.endswith(".shp"):
        shapefile_path = os.path.join(extract_dir, file)
        break  # Use the first .shp file found

if not shapefile_path:
    raise FileNotFoundError("No .shp file found in extracted directory.")

print(f"Using shapefile: {shapefile_path}")

# Step 3: Load the shapefile into GeoPandas
gdf_nyc = gpd.read_file(shapefile_path)

# Step 4: Convert to CRS with latitude/longitude (WGS 84) for correct mapping
gdf_nyc = gdf_nyc.to_crs(epsg=4326)

print(gdf_nyc.head())
```

```{python}
# Remove missing values for latitude and longitude
df_crashes = df.dropna(subset=['latitude', 'longitude'])

# Convert to GeoDataFrame
gdf_crashes = gpd.GeoDataFrame(df_crashes, 
                               geometry=gpd.points_from_xy(df_crashes.longitude, df_crashes.latitude),
                               crs="EPSG:4326")

print(gdf_crashes[['zip_code', 'latitude', 'longitude']])
```

```{python}
# Set up figure and axis
fig, ax = plt.subplots(figsize=(10, 12))

# Plot NYC boundary map
gdf_nyc.plot(ax=ax, cmap='viridis', 
linewidth=0.8, 
edgecolor='black',
legend=True, 
label="NYC Boundaries")

# Overlay crash locations as red dots
gdf_crashes.plot(ax=ax, color='red', markersize=2, alpha=0.6, label="Crash Locations")

# Add title and legend
ax.set_title("NYC Crash Locations Overlay on NYC Map", fontsize=14)
ax.legend()

# Remove axes
ax.set_xticks([])
ax.set_yticks([])
ax.set_frame_on(False)

# Show plot
plt.show()
```

    a.  Create a new variable `severe` which is one if the number of persons
     injured or deaths is 1 or more; and zero otherwise. Construct a cross
     table for `severe` versus borough. Is the severity of the crashes the
     same across boroughs? Test the null hypothesis that the two variables
     are not associated with an appropriate test.



    a.  Merge the crash data with the Census zip code database which
   contains zip-code level demographic or socioeconomic variables.



    a.  Fit a logistic model with `severe` as the outcome variable and covariates
     that are available in the data or can be engineered from the data. For
     example, zip code level covariates obtained from merging with the
     zip code database; crash hour; number of vehicles involved.