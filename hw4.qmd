---
title: "Homework Assignment 4"
author: "Shiyi Li"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---

# NYC Crash Data Exploration

Except for the first question, use the cleaned crash data in feather format.

    a.  Construct a contigency table for missing in geocode (latitude and
     longitude) by borough. Is the missing pattern the same across boroughs?
     Formulate a hypothesis and test it. 

```{python}
import pandas as pd
import numpy as np
import rpy2.robjects.numpy2ri
from rpy2.robjects.packages import importr
rpy2.robjects.numpy2ri.activate()

stats = importr('stats')

# Load the cleaned dataset exported from the 'Homework 4 - NYC Data Cleaning' assignment
file_path = 'data/shiyili_cleaned_data.csv'
df_shiyili = pd.read_csv(file_path)

# Create a 'geo_missing' column to show where rows are missing latitude and longitude.
df_shiyili['geo_missing'] = np.where(df_shiyili[['latitude', 'longitude']].isnull().all(axis=1)==True, 1, 0)

# Construct contingency table for missing geocodes by borough
contingency_table = pd.crosstab(df_shiyili['geo_missing'], df_shiyili['borough'])
print(contingency_table)

# Perform Fisher's exact test for independence
res = stats.fisher_test(contingency_table.to_numpy(), simulate_p_value = True)
print(res)

# Interpret results
alpha = 0.05  # Significance level
p = res[0][0]
if p < alpha:
    print("Reject the null hypothesis: Missing geocodes are NOT independent of boroughs.")
else:
    print("Fail to reject the null hypothesis: No strong evidence that missing geocodes depend on boroughs.")
```

    a.  Construct a `hour` variable with integer values from 0 to 23. Plot the
     histogram of the number of crashes by `hour`. Plot it by borough.

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Load the cleaned dataset
df = pd.read_feather("data/nyccrashes_cleaned.feather")

# Create a 'hour' column to store the hours extracted from the crash_datetime column
df['hour'] = pd.to_datetime(df['crash_datetime']).dt.hour

# Plot histogram of number of crashes by hour (all boroughs combined)
plt.figure(figsize=(10, 5))
plt.hist(df['hour'], bins=24, edgecolor='black', alpha=0.7)
plt.xlabel("Hour of the Day")
plt.ylabel("Number of Crashes")
plt.title("Histogram of NYC Crashes by Hour")
plt.xticks(range(0, 24))  # Ensure all hours are labeled
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Plot histogram of crashes by hour for each borough

plt.figure(figsize=(12, 6))
for borough in df['borough'].dropna().unique():
    subset = df[df['borough'] == borough]
    plt.hist(subset['hour'], bins=24, alpha=0.5, label=borough)

plt.xlabel("Hour of the Day")
plt.ylabel("Number of Crashes")
plt.title("Histogram of NYC Crashes by Hour and Borough")
plt.xticks(range(0, 24))
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Get unique boroughs
boroughs = df['borough'].dropna().unique()

# Define subplot grid size
n_rows = 2
n_cols = 3
fig, ax = plt.subplots(n_rows, n_cols, figsize=(20, 12))  # Adjusted for 6 boroughs max
ax = ax.flatten()  # Flatten the axes array for easier iteration

# Plot histogram of crashes by hour for each borough
for idx, borough_name in enumerate(boroughs):
    x = df[df['borough'] == borough_name]['hour']
    ax[idx].hist(x, bins=24, alpha=0.7, edgecolor='black')
    ax[idx].set_title(f'Histogram of NYC Crashes by Hour for {borough_name}')
    ax[idx].set_xlabel("Hour of the Day")
    ax[idx].set_ylabel("Number of Crashes")
    ax[idx].set_xticks(range(0, 24))
    ax[idx].grid(axis='y', linestyle='--', alpha=0.7)

# Hide unused subplots if boroughs < 6
for i in range(len(boroughs), len(ax)):
    fig.delaxes(ax[i])

plt.tight_layout()
plt.show()

```
     
    a.  Overlay the locations of the crashes on a map of NYC. The map could be a
     static map or a Google map.



    a.  Create a new variable `severe` which is one if the number of persons
     injured or deaths is 1 or more; and zero otherwise. Construct a cross
     table for `severe` versus borough. Is the severity of the crashes the
     same across boroughs? Test the null hypothesis that the two variables
     are not associated with an appropriate test.



    a.  Merge the crash data with the Census zip code database which
   contains zip-code level demographic or socioeconomic variables.



    a.  Fit a logistic model with `severe` as the outcome variable and covariates
     that are available in the data or can be engineered from the data. For
     example, zip code level covariates obtained from merging with the
     zip code database; crash hour; number of vehicles involved.